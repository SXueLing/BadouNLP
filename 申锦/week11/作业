#coding:utf8

import torch
import torch.nn as nn
import numpy as np
import math
import random
import json
import os
import re
from transformers import BertTokenizer, BertModel

"""
基于pytorch的LSTM语言模型
"""

class LanguageModel(nn.Module):
    def __init__(self, hidden_size, vocab_size, pretrain_model_path):  # 改成3个
        super(LanguageModel, self).__init__()
        self.bert = BertModel.from_pretrained(pretrain_model_path, return_dict=False)  # 模型地址
        self.classify = nn.Linear(hidden_size, vocab_size)  # 词表*维度
        self.loss = nn.functional.cross_entropy

    # 当输入真实标签，返回loss值；无真实标签，返回预测值
    def forward(self, x, y=None):
        outputs = self.bert(x)  # 不使用索引，直接获取整个输出元组
        x = outputs[0]  # 获取最后一层的隐藏状态
        y_pred = self.classify(x)

        if y is not None:
            return self.loss(y_pred.view(-1, y_pred.shape[-1]), y.view(-1))
        else:
            return y_pred

def load_corpus(path):
    samples = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            sample = json.loads(line.strip())
            if 'title' in sample and 'content' in sample:
                samples.append(sample)
    return samples

def build_sample(tokenizer, sample, question_end_token="？", max_length=100):  # 增加max_length
    try:
        question = sample['title']
        answer = sample['content']
        # 编码问题和答案
        x = tokenizer.encode(question, add_special_tokens=False, padding='max_length', truncation=True,
                             max_length=max_length)
        y = tokenizer.encode(answer, add_special_tokens=False, padding='max_length', truncation=True,
                             max_length=max_length)
        return x, y
    except Exception as e:
        print(f"Error processing sample: {e}")
        return None, None

# 建立数据集
# sample_length 输入需要的样本数量。需要多少生成多少
# vocab 词表
# window_size 样本长度
# corpus 语料字符串
def build_dataset(sample_length, tokenizer, samples, max_length=100):  # 增加max_length
    dataset_x = []
    dataset_y = []
    for i in range(min(sample_length, len(samples))):
        sample = samples[i]
        # 为每个JSON对象调用build_sample
        x, y = build_sample(tokenizer, sample, max_length=max_length)
        if x is not None and y is not None:
            dataset_x.append(x)
            dataset_y.append(y)
    return torch.LongTensor(dataset_x), torch.LongTensor(dataset_y)

# 建立模型
def build_model(vocab, char_dim, pretrain_model_path):
    model = LanguageModel(char_dim, vocab, pretrain_model_path)
    return model

# 文本生成测试代码
def generate_sentence(question, model, tokenizer, max_length=300):
    model.eval()
    with torch.no_grad():
        input_ids = tokenizer.encode(question, add_special_tokens=False)
        input_ids = torch.LongTensor([input_ids])
        if torch.cuda.is_available():
            input_ids = input_ids.cuda()

        generated_text = []
        for _ in range(max_length):
            outputs = model(input_ids)
            logits = outputs[:, -1, :]  # 获取最后一个时间步的输出
            probabilities = torch.softmax(logits, dim=-1)
            index = sampling_strategy(probabilities[0])

            generated_text.append(index)
            input_ids = torch.cat([input_ids, torch.tensor([[index]]).to(input_ids.device)], dim=1)

            # 如果生成了结束符，可以考虑提前结束
            # 这里简单假设结束符的id是tokenizer.sep_token_id
            if index == tokenizer.sep_token_id:
                break

        generated_text = tokenizer.decode(generated_text)
        return generated_text

def sampling_strategy(prob_distribution, top_p=0.9):  # 增加核采样
    sorted_probs, sorted_indices = torch.sort(prob_distribution, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    sorted_indices_to_remove = cumulative_probs > top_p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0

    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    prob_distribution[indices_to_remove] = 0

    if random.random() > 0.1:
        strategy = "greedy"
    else:
        strategy = "sampling"

    if strategy == "greedy":
        return int(torch.argmax(prob_distribution))
    elif strategy == "sampling":
        prob_distribution = prob_distribution.cpu().numpy()
        return np.random.choice(list(range(len(prob_distribution))), p=prob_distribution)

def train(corpus_path, pretrain_model_path, save_weight=True):
    epoch_num = 50        # 增加训练轮数
    batch_size = 64       # 调整batch_size
    train_sample = 100000   # 每轮训练总共训练的样本总数
    char_dim = 768        # 每个字的维度
    window_size = 100       # 增加样本文本长度
    vocab_size = 21128  # 字表大小
    learning_rate = 0.0001  # 降低学习率

    tokenizer = BertTokenizer.from_pretrained(pretrain_model_path, return_dict=False)

    samples = load_corpus(corpus_path)
    model = build_model(vocab_size, char_dim, pretrain_model_path)    # 建立模型

    if torch.cuda.is_available():
        model = model.cuda()
    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)   # 建立优化器
    print("文本词表模型加载完毕，开始训练")
    for epoch in range(epoch_num):
        model.train()
        watch_loss = []
        for batch in range(int(train_sample / batch_size)):
            x, y = build_dataset(batch_size, tokenizer, samples, max_length=window_size) # 构建一组训练样本
            if torch.cuda.is_available():
                x, y = x.cuda(), y.cuda()
            optim.zero_grad()    # 梯度归零
            loss = model(x, y)   # 计算loss
            loss.backward()      # 计算梯度
            optim.step()         # 更新权重
            watch_loss.append(loss.item())
        print("=========\n第%d轮平均loss:%f" % (epoch + 1, np.mean(watch_loss)))
        print(generate_sentence("阿根廷歹徒抢服装尺码不对拿回店里换", model, tokenizer))
    if not save_weight:
        return
    else:
        base_name = os.path.splitext(os.path.basename(corpus_path))[0] + ".pth"
        model_path = os.path.join("model", base_name)
        if not os.path.exists("model"):
            os.makedirs("model")
        torch.save(model.state_dict(), model_path)
        return

if __name__ == "__main__":
    corpus_path = f'E:\\pythonProject\\北京-学习资料\\week10\\week10 文本生成问题\\transformers-生成文章标题\\sample_data.json'
    pretrain_model_path = r"E:\pythonProject\北京-学习资料\week6\bert-base-chinese"
    train(corpus_path, pretrain_model_path, False)
