
> 在NNLM模型中使用BERT+mask，训练一个自回归的seqSeq模型（`decoder only`）

**数据加载**

1. 数据的输入与输出分别为等长的错位序列
2. 使用`bert`对应的分词器


**模型**

1. 使用`bert+mask`进行获取前向信息，并预测后序信息

**生成预测**

1. 基于前一部分数据，生成单个预测的字，并将该字追加到字符串尾部
2. 从一系列预测字中进行选择（采样），可以采用贪婪，也可以随机，或二者结合（在top-k中进行选择）

**具体细节**

- 在数据加载部分，使用`bert`自带的分词，使用`encode_plus`获取分词后更为全面的信息，数据集的每一条项目是：`{'inputs':{x.items()},'labels':{y.items()}}`
- 模型运行部分，使用`bert`作为编码层，给`bert`添加一个掩码`attention_mask`,达到自回归效果。具体前向传播方面主要，此时数据是<font color=red>字典形式，对输入进行解包传播模型，对标签需要提取核心的数据</font>